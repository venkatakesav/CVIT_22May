{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Recognition Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code\n",
    "\n",
    "First, let's load the MNIST dataset and split it into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST data from https://openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: PCA + KNN\n",
    "\n",
    "### Prelude\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique to emphasize variation and bring out strong patterns in a dataset. It's often used to make data easy to explore and visualize. Here, you will use PCA to reduce the dimensionality of the MNIST dataset before applying the KNN algorithm for classification.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Load the Dataset:** Start by loading the MNIST dataset.\n",
    "2. **Apply PCA:** Reduce the dimensionality of the dataset.\n",
    "3. **KNN Classification:** Use the KNN algorithm to classify the digits.\n",
    "\n",
    "# Tips\n",
    "- Choosing n_components for PCA: Start with n_components=0.95 which keeps 95% of the variance. Experiment with other values to see how it changes the results.\n",
    "- Choosing n_neighbors for KNN: Common starting points are 3, 5, and 7. Adjust based on the performance and try to avoid overfitting.\n",
    "- Explore: Use visualizations like plotting some of the digits before and after PCA to understand what is retained and what is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: K-Means + SVM\n",
    "\n",
    "### Prelude\n",
    "\n",
    "K-Means is a popular clustering algorithm, and Support Vector Machines (SVMs) are a powerful classification method. In this part, you will use K-Means to extract features from the dataset and then use these features to train an SVM classifier.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **K-Means Clustering:** Apply K-Means to find clusters in the dataset.\n",
    "2. **Feature Extraction:** Use the distances from each point to the cluster centroids as features.\n",
    "3. **SVM Classification:** Use the SVM classifier to classify the digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Tips for Students:**\n",
    "- **Choosing the Number of Clusters (k) in K-Means:** Start with `k=10` since there are 10 digits (0-9). Experiment with different values to see if they improve the performance.\n",
    "- **Selecting SVM Kernel:** Try different kernels like 'linear', 'poly', 'rbf', and 'sigmoid'. Observe how the choice of kernel affects accuracy.\n",
    "- **Visualization:** Consider visualizing the centroids of the clusters. Each centroid is a point in the same space as the input data and can be viewed as an \"average\" digit if reshaped to 28x28 pixels.\n",
    "- **Cross-Validation:** Use cross-validation to find the best parameters for both K-Means and SVM to further improve the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: SIFT + SVM\n",
    "\n",
    "### Prelude\n",
    "\n",
    "Scale-Invariant Feature Transform (SIFT) is an algorithm to detect and describe local features in images. After extracting these features, you will use an SVM classifier for the classification.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **SIFT Feature Extraction:** Extract SIFT features from each image.\n",
    "2. **Feature Description:** Use the features to describe the dataset.\n",
    "3. **SVM Classification:** Use these descriptions to train and predict using SVM.\n",
    "\n",
    "### Starter Code\n",
    "\n",
    "#### SIFT Feature Extraction\n",
    "\n",
    "First, let's define a function to extract SIFT features from an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Tips for Students:**\n",
    "- **SIFT Feature Size:** SIFT descriptors are 128-dimensional; ensure all feature vectors are the same length.\n",
    "- **Choosing SVM Kernel:** Try 'linear', 'poly', 'rbf', and 'sigmoid' kernels to observe their effects.\n",
    "- **Regularization Parameter (C):** Experiment with different values of \\(C\\); smaller values specify stronger regularization.\n",
    "- **Handling Missing Descriptors:** In case no keypoints are found in an image, use a zero vector for that imageâ€™s descriptors.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
